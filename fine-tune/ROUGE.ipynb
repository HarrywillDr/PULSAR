{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5_auWyhkc9s",
        "outputId": "bd5b987f-3976-48af-b50a-5ca348035d35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge-score) (8.1.3)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=9e8c9fe307988a584dbbd9e421e7cd03832ce8479efc87e9af35a2f32a392b9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DdW0lrV0Zo8k"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "from itertools import product\n",
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i1zfebWl5g3",
        "outputId": "bde9c065-2f47-4239-cef8-2a9c5546f0f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset_path = '/content/drive/MyDrive/BioNLP-2023/bionlp-workshop-2023-shared-task-1a-problem-list-summarization-1.0.0/'"
      ],
      "metadata": {
        "id": "0oO5Mlqdklks"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pagerank predictions\n",
        "ground_truth = pd.read_csv(testset_path + \"BioNLP2023-1A-newTest.csv\")"
      ],
      "metadata": {
        "id": "cLZxVrYukPOp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_file = 'system.jsonl'\n",
        "txt_file = 'system.txt'\n",
        "p = inflect.engine()\n",
        "\n",
        "with open(jsonl_file, 'r') as f_in, open(txt_file, 'w') as f_out:\n",
        "    for line in f_in:\n",
        "        data = json.loads(line)\n",
        "        words = data.split()\n",
        "        singular_words = []\n",
        "        for word in words:\n",
        "            singular_word = p.singular_noun(word) or word\n",
        "            singular_words.append(singular_word)\n",
        "        singular_data = ' '.join(singular_words)\n",
        "        # Remove all the \"#\" characters\n",
        "        #singular_data = singular_data.replace(\"#\", \"\")\n",
        "        f_out.write(singular_data + '\\n')"
      ],
      "metadata": {
        "id": "-ZxR-olo6csM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import inflect\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def add_txt_column(df, txt_file, column_name):\n",
        "    \"\"\"\n",
        "    Adds a text file as a new column to a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): DataFrame to add the column to.\n",
        "        txt_file (str): Path to the text file.\n",
        "        column_name (str): Name of the new column.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with the new column.\n",
        "    \"\"\"\n",
        "    with open(txt_file, 'r') as f:\n",
        "        data = f.read().splitlines()\n",
        "\n",
        "    df[column_name] = data\n",
        "\n",
        "    return df\n",
        "\n",
        "def pairwise_rouge(df, column_name_1, column_name_2):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    scores = []\n",
        "    for i in range(len(df)):\n",
        "        score = scorer.score(df.iloc[i][column_name_1], df.iloc[i][column_name_2])\n",
        "        \n",
        "        scores.append({\n",
        "            \"document\": i,\n",
        "            \"rouge_1\": score[\"rouge1\"].fmeasure,\n",
        "            \"rouge_2\": score[\"rouge2\"].fmeasure,\n",
        "            \"rouge_L\": score[\"rougeL\"].fmeasure\n",
        "        })\n",
        "        \n",
        "    return scores"
      ],
      "metadata": {
        "id": "iETJHi_-sLAt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add a JSONL file as a column to the DataFrame\n",
        "df = add_txt_column(ground_truth, \"system.txt\", \"generated_summary\")\n",
        "\n",
        "# compare the contents of the \"summary\" column pairwise using Rouge\n",
        "scores = pairwise_rouge(df, \"summary\", \"generated_summary\")\n",
        "\n",
        "# print the pairwise Rouge scores\n",
        "for score in scores:\n",
        "    print(score)"
      ],
      "metadata": {
        "id": "d_nnkdKfigSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(testset_path + \"BioNLP2023-1A-newTest_compare.csv\", index=False)"
      ],
      "metadata": {
        "id": "AKDPiHw_nsyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =pd.read_csv(testset_path + \"BioNLP2023-1A-newTest_compare.csv\")"
      ],
      "metadata": {
        "id": "MZuH02_E6O2-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the contents of the \"summary\" column pairwise using Rouge\n",
        "scores = pairwise_rouge(df, \"Summary\", \"generated_summary\")"
      ],
      "metadata": {
        "id": "BRui5FvE6TXM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the average Rouge scores\n",
        "rouge_1_scores = [score[\"rouge_1\"] for score in scores]\n",
        "rouge_2_scores = [score[\"rouge_2\"] for score in scores]\n",
        "rouge_l_scores = [score[\"rouge_L\"] for score in scores]\n",
        "\n",
        "avg_rouge_1 = np.mean(rouge_1_scores)\n",
        "avg_rouge_2 = np.mean(rouge_2_scores)\n",
        "avg_rouge_l = np.mean(rouge_l_scores)\n",
        "\n",
        "print(\"Average Rouge-1 score:\", avg_rouge_1)\n",
        "print(\"Average Rouge-2 score:\", avg_rouge_2)\n",
        "print(\"Average Rouge-L score:\", avg_rouge_l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZdMrgnV6e2F",
        "outputId": "1ee24b4b-b9f7-4afe-d5ae-e26cdc51c246"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Rouge-1 score: 0.27262912804913153\n",
            "Average Rouge-2 score: 0.09779431087128522\n",
            "Average Rouge-L score: 0.23630261669371613\n"
          ]
        }
      ]
    }
  ]
}